# -*- coding: utf-8 -*-
"""Project Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xtKfYDtUnnfmon0vc0MYLskInHD53EDx
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Imported the train and test datasets
train_df = pd.read_csv('/content/drive/MyDrive/MIS 545/Project/train.csv')
test_df = pd.read_csv('/content/drive/MyDrive/MIS 545/Project/test.csv')

train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]
test_df = test_df.loc[:, ~test_df.columns.str.contains('^Unnamed')]

train_df.head()

# Printed the rows and columns of train and test data
train_df.shape, test_df.shape

#Count the number of null values in train_df
train_df.isnull().sum()

#Replacing the null values in the train and test sets with the median from the train set
train_df['Arrival Delay in Minutes'].fillna(train_df['Arrival Delay in Minutes'].median(), inplace=True)
test_df['Arrival Delay in Minutes'].fillna(train_df['Arrival Delay in Minutes'].median(), inplace=True)

#Checking for nulls in the train set
train_df.isnull().sum()

# Encoding the categorical variables

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder

nominal_cols = ['Gender', 'Type of Travel']
ordinal_cols = ['Customer Type', 'Class']

ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)
oe = OrdinalEncoder(categories=[['disloyal Customer', 'Loyal Customer'],['Eco', 'Eco Plus', 'Business']])
le = LabelEncoder()

train_nominal = pd.DataFrame(ohe.fit_transform(train_df[nominal_cols]),
                             columns=ohe.get_feature_names_out(nominal_cols),
                             index=train_df.index)

train_ordinal = pd.DataFrame(oe.fit_transform(train_df[ordinal_cols]),
                             columns=ordinal_cols,
                             index=train_df.index)

train_df['satisfaction'] = le.fit_transform(train_df['satisfaction'])

test_nominal = pd.DataFrame(ohe.transform(test_df[nominal_cols]),
                            columns=ohe.get_feature_names_out(nominal_cols),
                            index=test_df.index)

test_ordinal = pd.DataFrame(oe.transform(test_df[ordinal_cols]),
                            columns=ordinal_cols,
                            index=test_df.index)

test_df['satisfaction'] = le.transform(test_df['satisfaction'])

train_final = pd.concat([train_df.drop(columns=nominal_cols + ordinal_cols),
                         train_nominal, train_ordinal], axis=1)

test_final = pd.concat([test_df.drop(columns=nominal_cols + ordinal_cols),
                        test_nominal, test_ordinal], axis=1)

X_train = train_final.drop(columns=['satisfaction', 'id'])
y_train = train_final['satisfaction']

X_test = test_final.drop(columns=['satisfaction', 'id'])
y_test = test_final['satisfaction']

#Check for missing values in X_train
X_train.isnull().sum()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Base Logistic Regression Model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score

logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_scaled, y_train)

y_pred = logreg.predict(X_test_scaled)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))

#Running LASSO Regression
from sklearn.linear_model import LogisticRegressionCV

lasso = LogisticRegressionCV(
    Cs=10,
    cv=5,
    penalty='l1',
    solver='liblinear',
    max_iter=1000
)
lasso.fit(X_train_scaled, y_train)

print("Best C (inverse of regularization strength):", lasso.C_)
print("Test Accuracy:", lasso.score(X_test_scaled, y_test))
#Print Precision
print("Precision:", precision_score(y_test, lasso.predict(X_test_scaled)))
#Print Recall
print("Recall:", recall_score(y_test, lasso.predict(X_test_scaled)))

#Features that have been retained
selected_features = X_train.columns[(lasso.coef_ != 0).ravel()]
print("Selected features after LASSO:\n", selected_features)

from sklearn.feature_selection import RFECV

rfecv = RFECV(
    estimator=LogisticRegression(max_iter=1000),
    step=1,
    cv=5,
    scoring='accuracy'
)
rfecv.fit(X_train_scaled, y_train)

print("Optimal number of features:", rfecv.n_features_)
print("Best features:", X_train.columns[rfecv.support_])

#Print Accuracy for RFECV
print("Accuracy:", rfecv.score(X_test_scaled, y_test))
#Print Precision
print("Precision:", precision_score(y_test, rfecv.predict(X_test_scaled)))
#Print Recall
print("Recall:", recall_score(y_test, rfecv.predict(X_test_scaled)))

"""Base Logistic Regression:
*   Accuracy: 0.8713427779488759
*   Precision: 0.8680485800383526
*   Recall: 0.8336402701043585

LASSO:
*   Accuracy: 0.8711887896519864
*   Precision: 0.8678659483152223
*   Recall: 0.8334648776637726

RFECV:
*   Accuracy: 0.8704188481675392
*   Precision: 0.8664174341205434
*   Recall: 0.8332894852231869
"""

import matplotlib.pyplot as plt
import numpy as np

coef = pd.Series(lasso.coef_.ravel(), index=X_train.columns)
coef.sort_values().plot(kind='barh', figsize=(8, 10))
plt.title('Feature Importance (LASSO Logistic Regression)')
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)
import time

#Declaring our models and hyperparameters
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, solver='liblinear', random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'XGBoost': XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)
}

param_grids = {
    'Logistic Regression': {'C': [0.1, 1, 10]},
    'Random Forest': {'n_estimators': [100, 200], 'max_depth': [10, 20], 'min_samples_split': [2, 5]},
    'XGBoost': {'n_estimators': [100, 200], 'max_depth': [4, 6], 'learning_rate': [0.1, 0.3]}
}

results = []

#Model training and evaluation
for name, model in models.items():
    print(f"\nTraining {name}...")
    start = time.time()

    grid = GridSearchCV(model, param_grids[name], cv=3, scoring='accuracy', n_jobs=-1, verbose=1)
    grid.fit(X_train, y_train)

    best_model = grid.best_estimator_
    y_pred = best_model.predict(X_test)
    y_prob = best_model.predict_proba(X_test)[:, 1]

    #Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average='binary')
    rec = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')
    auc = roc_auc_score(y_test, y_prob)

    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1,
        'ROC-AUC': auc,
        'Training Time (s)': round(time.time() - start, 2),
        'Best Params': grid.best_params_
    })

#Comparison table
results_df = pd.DataFrame(results)
print("\nModel Comparison Table:")
display(results_df)

#Best model by accuracy
best_model_name = results_df.loc[results_df['Accuracy'].idxmax(), 'Model']
print(f"\nBest Model by Accuracy: {best_model_name}")

#Best model by AUC
best_model_auc = results_df.loc[results_df['ROC-AUC'].idxmax(), 'Model']
print(f"Best Model by ROC-AUC: {best_model_auc}")

#Print best params
print("\nBest Parameters for Each Model:")
for _, row in results_df.iterrows():
    print(f"  {row['Model']}: {row['Best Params']}")

import shap
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression

# Identify best model based on accuracy
best_model_name = results_df.loc[results_df['Accuracy'].idxmax(), 'Model']
print(f" Best Model Selected for SHAP: {best_model_name}")

# Refit the best model on the full training data using its best params
best_params = results_df.loc[results_df['Accuracy'].idxmax(), 'Best Params']
model_class = models[best_model_name].__class__
best_model = model_class(**best_params, random_state=42)

# Use scaled data for training the best model as the models were evaluated on scaled data
best_model.fit(X_train_scaled, y_train)
print(" Best model retrained on full training data.")

import shap

# Sample small subset for speed
X_test_sample = X_test.sample(100, random_state=42)
X_train_sample = X_train.sample(100, random_state=42)  # background for KernelExplainer

# Define prediction function
f = lambda x: best_model.predict_proba(x)[:, 1]

# Create KernelExplainer (model-agnostic)
explainer = shap.KernelExplainer(f, X_train_sample)

# Compute SHAP values (this may take a few minutes)
shap_values = explainer.shap_values(X_test_sample, nsamples=100)

# Plot SHAP summary
shap.summary_plot(shap_values, X_test_sample)
shap.summary_plot(shap_values, X_test_sample, plot_type="bar")

import pandas as pd
import numpy as np

# If KernelExplainer returns array
mean_abs_shap = np.abs(shap_values).mean(axis=0)

# Create a DataFrame
feature_importance = pd.DataFrame({
    'Feature': X_test_sample.columns,
    'Mean_ABS_SHAP': mean_abs_shap
}).sort_values(by='Mean_ABS_SHAP', ascending=False)

# Top 10 features
top_features = feature_importance.head(10)
print(top_features)

"""
### Detailed Explanation of Each Step:

1.  **Data Loading & Initial Inspection**:
    *   **Action**: The process begins by mounting Google Drive to access the datasets. Then, `train.csv` and `test.csv` are loaded into pandas DataFrames. Any 'Unnamed' columns are removed, and the first few rows of the training data (`train_df.head()`) are displayed along with the dimensions (rows, columns) of both datasets (`train_df.shape, test_df.shape`) to get a quick overview.
    *   **Purpose**: To import the necessary data and understand its basic structure and size.

2.  **Handle Missing Values**:
    *   **Action**: The code checks for null values in `train_df`. Specifically, it identifies missing values in the 'Arrival Delay in Minutes' column. These missing values are then imputed (filled) with the median value of 'Arrival Delay in Minutes' from the training set, applying this to both the training and test datasets. Finally, it re-verifies that there are no remaining nulls in `train_df`.
    *   **Purpose**: To ensure data completeness and prevent errors or biases in subsequent modeling steps due to missing information.

3.  **Feature Engineering: Encoding Categorical Variables**:
    *   **Action**: Categorical features are transformed into numerical representations. `OneHotEncoder` is applied to nominal variables ('Gender', 'Type of Travel'), `OrdinalEncoder` to ordinal variables ('Customer Type', 'Class'), and `LabelEncoder` to the target variable 'satisfaction'. The original categorical columns are dropped, and the newly encoded numerical columns are concatenated to form `train_final` and `test_final` DataFrames.
    *   **Purpose**: Machine learning algorithms typically require numerical input, so this step converts non-numerical features into a suitable format.

4.  **Data Splitting: X and y**:
    *   **Action**: The processed datasets (`train_final` and `test_final`) are split into features (`X_train`, `X_test`) and the target variable (`y_train`, `y_test`). The 'satisfaction' column is the target, and the 'id' column is dropped as it's an identifier, not a feature.
    *   **Purpose**: To prepare the data for supervised learning, separating the independent variables (features) from the dependent variable (target).

5.  **Feature Scaling: StandardScaler**:
    *   **Action**: `StandardScaler` is applied to the feature sets (`X_train` and `X_test`). This scales the features such that they have a mean of 0 and a standard deviation of 1.
    *   **Purpose**: Scaling is crucial for many machine learning algorithms (especially distance-based ones like Logistic Regression) to ensure that no single feature dominates the learning process due to its larger numerical range.

6.  **Model Training & Evaluation: Baseline, LASSO, RFECV**:
    *   **Action**: Three different logistic regression-based models are trained and evaluated:
        *   **Base Logistic Regression**: A standard Logistic Regression model, with L2 regularization, is trained and its accuracy, precision, and recall are reported.
        *   **LASSO Regression**: Logistic Regression with L1 regularization (LASSO) is applied using `LogisticRegressionCV` to perform cross-validation and select the optimal regularization strength. The best C value (inverse of regularization strength), accuracy, precision, and recall are printed, along with the features retained by LASSO.
        *   **RFECV**: Recursive Feature Elimination with Cross-Validation (`RFECV`) is used with a Logistic Regression estimator to identify the optimal number of features and the best subset of features. The accuracy, precision, and recall of the model with these selected features are reported.
    *   **Purpose**: To establish baseline model performance, explore regularization for feature selection (LASSO), and perform automated feature selection (RFECV) to potentially improve model efficiency and performance.

7.  **Visualize LASSO Feature Importance**:
    *   **Action**: The coefficients of the LASSO regression model are extracted, paired with their corresponding feature names, and then sorted. A horizontal bar plot is generated to visually represent the importance (magnitude of coefficients) of each feature as determined by LASSO.
    *   **Purpose**: To provide a clear visualization of which features were deemed most influential by the LASSO model in predicting satisfaction.

8.  **Model Comparison & Hyperparameter Tuning (GridSearchCV)**:
    *   **Action**: Logistic Regression, Random Forest, and XGBoost models are set up. For each model, `GridSearchCV` is used to systematically search for the best combination of hyperparameters that maximize accuracy through cross-validation. Metrics including Accuracy, Precision, Recall, F1 Score, and training time are recorded. The results are saved to a CSV and displayed in a DataFrame, highlighting the best model based on accuracy and its optimal hyperparameters.
    *   **Purpose**: To find the best-performing model among several candidates and optimize their performance by tuning their hyperparameters, leading to a more robust and accurate predictive model.

9.  **Retrain Best Model & SHAP Feature Importance**:
    *   **Action**: The best model identified from the hyperparameter tuning phase is retrained on the full scaled training data using its optimal parameters. Then, SHAP (SHapley Additive exPlanations) is used to interpret this best model. A small sample of `X_test` and `X_train` is used for efficiency. A `shap.KernelExplainer` is initialized, and SHAP values are computed. Finally, SHAP summary plots (dot plot and bar plot) are generated to visualize feature importance, and the top 10 features based on their mean absolute SHAP values are printed.
    *   **Purpose**: To provide deep insights into how the best model makes its predictions, explaining the contribution of each feature to the model's output in a human-understandable way."""